{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Before you submit this notebook, make sure everything runs as expected in the local test cases. \nPlease, paste the solution to the designed cell and do not change anything else.\n\nAlso, please, leave your first and last names below",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "FirstName = \"Anna\"\nLastName = \"Markelova\"",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "---",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom sklearn.base import BaseEstimator\nfrom sklearn.preprocessing import OneHotEncoder",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def entropy(y):  \n    \"\"\"\n    Computes entropy of the provided distribution. Use log(value + eps) for numerical stability\n    \n    Parameters\n    ----------\n    y : np.array of type float with shape (n_objects, n_classes)\n        One-hot representation of class labels for corresponding subset\n    \n    Returns\n    -------\n    float\n        Entropy of the provided subset\n    \"\"\"\n    EPS = 0.0005\n    E = 0.\n    for i in range(y.shape[1]):\n        E -= float(np.sum(y.T[i]) / len(y) * np.log(np.sum(y.T[i]) / len(y) + EPS))\n    return E\n    \ndef gini(y):\n    \"\"\"\n    Computes the Gini impurity of the provided distribution\n    \n    Parameters\n    ----------\n    y : np.array of type float with shape (n_objects, n_classes)\n        One-hot representation of class labels for corresponding subset\n    \n    Returns\n    -------\n    float\n        Gini impurity of the provided subset\n    \"\"\"\n    G = 1.\n    for i in range(y.shape[1]):\n        G -= float(np.sum(y.T[i]) / len(y)) ** 2\n    return G \n    \ndef variance(y):\n    \"\"\"\n    Computes the variance the provided target values subset\n    \n    Parameters\n    ----------\n    y : np.array of type float with shape (n_objects, 1)\n        Target values vector\n    \n    Returns\n    -------\n    float\n        Variance of the provided target vector\n    \"\"\"\n    return np.mean((y - np.mean(y)) ** 2)\n\n\ndef mad_median(y):\n    \"\"\"\n    Computes the mean absolute deviation from the median in the\n    provided target values subset\n    \n    Parameters\n    ----------\n    y : np.array of type float with shape (n_objects, 1)\n        Target values vector\n    \n    Returns\n    -------\n    float\n        Mean absolute deviation from the median in the provided vector\n    \"\"\"\n    return  np.mean(np.abs(y - np.median(y)))\n\n\ndef one_hot_encode(n_classes, y):\n    y_one_hot = np.zeros((len(y), n_classes), dtype=float)\n    y_one_hot[np.arange(len(y)), y.astype(int)[:, 0]] = 1.\n    return y_one_hot\n\n\ndef one_hot_decode(y_one_hot):\n    return y_one_hot.argmax(axis=1)[:, None]\n\n\nclass Node:\n    \"\"\"\n    This class is provided \"as is\" and it is not mandatory to it use in your code.\n    \"\"\"\n    def __init__(self, feature_index, threshold, proba=0):\n        self.feature_index = feature_index\n        self.value = threshold\n        self.proba = proba\n        self.left_child = None\n        self.right_child = None\n        \n        \nclass Leaf:\n    def __init__(self, y, classification = True, criterion_name = None):\n        def count(y): #Counts the number of each type of example in a dataset\n            counts = {}  \n            y = one_hot_decode(y)\n            for label in y.T[0]:\n                if label not in counts: counts[label] = 0\n                counts[label] += 1\n            return counts     \n        if classification: \n            self.predictions = count(y)\n        else: \n            if criterion_name == 'mad_median':\n                self.predictions = np.median(y)\n            if criterion_name == 'variance': self.predictions = np.mean(y)\n\n\nclass DecisionTree(BaseEstimator):\n    all_criterions = {\n        'gini': (gini, True), # (criterion, classification flag)\n        'entropy': (entropy, True),\n        'variance': (variance, False),\n        'mad_median': (mad_median, False)\n    }\n\n    def __init__(self, n_classes=None, max_depth=np.inf, min_samples_split=2, \n                 criterion_name='gini', debug=False):\n\n        assert criterion_name in self.all_criterions.keys(), 'Criterion name must be on of the following: {}'.format(self.all_criterions.keys())\n        \n        self.n_classes = n_classes\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.criterion_name = criterion_name\n\n        self.depth = 0\n        self.root = None # Use the Node class to initialize it later\n        self.debug = debug\n\n        \n        \n    def make_split(self, feature_index, threshold, X_subset, y_subset):\n        \"\"\"\n        Makes split of the provided data subset and target values using provided feature and threshold\n        \n        Parameters\n        ----------\n        feature_index : int\n            Index of feature to make split with\n        threshold : float\n            Threshold value to perform split\n        X_subset : np.array of type float with shape (n_objects, n_features)\n            Feature matrix representing the selected subset\n        y_subset : np.array of type float with shape (n_objects, n_classes) in classification \n                   (n_objects, 1) in regression \n            One-hot representation of class labels for corresponding subset\n        \n        Returns\n        -------\n        (X_left, y_left) : tuple of np.arrays of same type as input X_subset and y_subset\n            Part of the providev subset where selected feature x^j < threshold\n        (X_right, y_right) : tuple of np.arrays of same type as input X_subset and y_subset\n            Part of the providev subset where selected feature x^j >= threshold\n        \"\"\"\n        X_left, y_left = X_subset[X_subset[:, feature_index] < threshold], y_subset[np.where(X_subset[:, feature_index] < threshold)] \n        X_right, y_right = X_subset[X_subset[:, feature_index] >= threshold], y_subset[np.where(X_subset[:, feature_index] >= threshold)] \n        return (X_left, y_left), (X_right, y_right)\n    \n    def make_split_only_y(self, feature_index, threshold, X_subset, y_subset):\n        \"\"\"\n        Split only target values into two subsets with specified feature and threshold\n        \n        Parameters\n        ----------\n        feature_index : int\n            Index of feature to make split with\n        threshold : float\n            Threshold value to perform split\n        X_subset : np.array of type float with shape (n_objects, n_features)\n            Feature matrix representing the selected subset\n        y_subset : np.array of type float with shape (n_objects, n_classes) in classification \n                   (n_objects, 1) in regression \n            One-hot representation of class labels for corresponding subset\n        \n        Returns\n        -------\n        y_left : np.array of type float with shape (n_objects_left, n_classes) in classification \n                   (n_objects, 1) in regression \n            Part of the provided subset where selected feature x^j < threshold\n        y_right : np.array of type float with shape (n_objects_right, n_classes) in classification \n                   (n_objects, 1) in regression \n            Part of the provided subset where selected feature x^j >= threshold\n        \"\"\"\n        y_left = y_subset[np.where(X_subset[:, feature_index] < threshold)] \n        y_right = y_subset[np.where(X_subset[:, feature_index] >= threshold)] \n        return y_left, y_right\n\n    def choose_best_split(self, X_subset, y_subset):\n        \"\"\"\n        Greedily select the best feature and best threshold w.r.t. selected criterion\n        \n        Parameters\n        ----------\n        X_subset : np.array of type float with shape (n_objects, n_features)\n            Feature matrix representing the selected subset\n        y_subset : np.array of type float with shape (n_objects, n_classes) in classification \n                   (n_objects, 1) in regression \n            One-hot representation of class labels or target values for corresponding subset\n        \n        Returns\n        -------\n        feature_index : int\n            Index of feature to make split with\n        threshold : float\n            Threshold value to perform split\n        \"\"\"\n        def compute(y_left, y_right, y_subset):\n            self.criterion = self.all_criterions[self.criterion_name][0]\n            p = len(y_left) / len(y_subset)\n            return self.criterion(y_subset) - (p * self.criterion(y_left) + (1 - p) * self.criterion(y_right)) \n        \n        max_criterion = 0.\n        feature_index = 0\n        threshold = 0.\n        for feature in range(X_subset.shape[1]):\n            featured_column = set(X_subset.T[feature])\n            for i in featured_column:\n                y_left, y_right = self.make_split_only_y(feature, i, X_subset, y_subset)\n                if len(y_left) == 0 or len(y_right) == 0: \n                    continue\n                curr_criterion = compute(y_left, y_right, y_subset)                \n                if curr_criterion > max_criterion:\n                    max_criterion = curr_criterion\n                    threshold = i \n                    feature_index = feature\n        return feature_index, threshold\n    \n    def make_tree(self, X_subset, y_subset):\n        \"\"\"\n        Recursively builds the tree\n        \n        Parameters\n        ----------\n        X_subset : np.array of type float with shape (n_objects, n_features)\n            Feature matrix representing the selected subset\n        y_subset : np.array of type float with shape (n_objects, n_classes) in classification \n                   (n_objects, 1) in regression \n            One-hot representation of class labels or target values for corresponding subset\n        \n        Returns\n        -------\n        root_node : Node class instance\n            Node of the root of the fitted tree\n        \"\"\"\n        def compute_split(node, X_subset, y_subset, depth):\n            (X_left, y_left), (X_right, y_right) = self.make_split(node.feature_index, node.value, X_subset, y_subset)\n            n_objects = len(y_subset.copy())\n            del(X_subset); del(y_subset)\n            if len(X_left) == 0 and len(X_right) == 0:    \n                node.left_child = node.right_child = Leaf(np.concatenate([y_left, y_right]), self.classification, self.criterion_name)\n                return \n            if depth >= self.max_depth: \n                node.left_child = Leaf(y_left, self.classification, self.criterion_name)\n                node.right_child = Leaf(y_right, self.classification, self.criterion_name)\n                return \n            if len(X_left) <= self.min_samples_split:\n                node.left_child = Leaf(y_left, self.classification, self.criterion_name) \n            else:\n                lfi, lthr = self.choose_best_split(X_left, y_left)\n                node.left_child = Node(lfi, lthr, proba = len(X_left) / n_objects)\n                compute_split(node.left_child, X_left, y_left, depth + 1)\n            if len(X_right) <= self.min_samples_split: \n                node.right_child = Leaf(y_right, self.classification, self.criterion_name) \n            else:\n                rfi, rthr = self.choose_best_split(X_right, y_right)\n                node.right_child = Node(rfi, rthr, proba = len(X_right) / n_objects)\n\n                compute_split(node.right_child, X_right, y_right, depth + 1)\n\n        #creating root\n        feature_index, threshold = self.choose_best_split(X_subset, y_subset) \n        root_node = Node(feature_index, threshold)       \n        compute_split(root_node, X_subset.copy(), y_subset.copy(), depth = 1)\n        self.depth = self.max_depth\n\n        return root_node\n        \n    def fit(self, X, y):\n        \"\"\"\n        Fit the model from scratch using the provided data\n        \n        Parameters\n        ----------\n        X : np.array of type float with shape (n_objects, n_features)\n            Feature matrix representing the data to train on\n        y : np.array of type int with shape (n_objects, 1) in classification \n                   of type float with shape (n_objects, 1) in regression \n            Column vector of class labels in classification or target values in regression\n        \n        \"\"\"\n        assert len(y.shape) == 2 and len(y) == len(X), 'Wrong y shape'\n        self.criterion, self.classification = self.all_criterions[self.criterion_name]\n        if self.classification:\n            if self.n_classes is None:\n                self.n_classes = len(np.unique(y))\n            y = one_hot_encode(self.n_classes, y)\n\n        self.root = self.make_tree(X, y)\n    \n    def predict(self, X):\n        \"\"\"\n        Predict the target value or class label  the model from scratch using the provided data\n        \n        Parameters\n        ----------\n        X : np.array of type float with shape (n_objects, n_features)\n            Feature matrix representing the data the predictions should be provided for\n        Returns\n        -------\n        y_predicted : np.array of type int with shape (n_objects, 1) in classification \n                   (n_objects, 1) in regression \n            Column vector of class labels in classification or target values in regression\n        \n        \"\"\"\n        def compute_prediction(row, node = self.root):\n            \"\"\" \n            Recursevly computes predictions, comparing obj values with threshold\n            \"\"\"\n            if row[node.feature_index] < node.value:\n                if isinstance(node.left_child, Leaf):\n                    if self.classification: \n                        return max(node.left_child.predictions, key = lambda x: node.left_child.predictions[x])\n                    return node.left_child.predictions      \n                elif isinstance(node.left_child, Node):\n                        return compute_prediction(row, node.left_child)\n            else:\n                if isinstance(node.right_child, Leaf):\n                    if self.classification: \n                        return max(node.right_child.predictions, key = lambda x: node.right_child.predictions[x])\n                    return node.right_child.predictions\n\n                elif isinstance(node.right_child, Node): \n                    return compute_prediction(row, node.right_child)\n        y_predicted = []\n        for x in X:\n            y_predicted.append(compute_prediction(x))\n        return y_predicted\n        \n    def predict_proba(self, X):\n        \"\"\"\n        Only for classification\n        Predict the class probabilities using the provided data\n        \n        Parameters\n        ----------\n        X : np.array of type float with shape (n_objects, n_features)\n            Feature matrix representing the data the predictions should be provided for\n        Returns\n        -------\n        y_predicted_probs : np.array of type float with shape (n_objects, n_classes)\n            Probabilities of each class for the provided objects\n        \n        \"\"\"\n        assert self.classification, 'Available only for classification problem'\n\n        def compute_proba(row, node = self.root):\n            if row[node.feature_index] < node.value:\n                if isinstance(node.left_child, Leaf):\n                    return  {k: v / total for total in (sum(node.left_child.predictions.values(), 0.0),) for k, v in node.left_child.predictions.items()}\n                elif isinstance(node.left_child, Node):\n                    return compute_proba(row, node.left_child)\n            else:\n                if isinstance(node.right_child, Leaf):\n                    return  {k: v / total for total in (sum(node.right_child.predictions.values(), 0.0),) for k, v in node.right_child.predictions.items()}      \n                elif isinstance(node.right_child, Node): \n                    return compute_proba(row, node.right_child)\n        y_predicted_probs = np.zeros((len(X), self.n_classes))\n        for ind in range(len(X)):\n            obj = compute_proba(X[ind])\n            probs = np.zeros((1, self.n_classes))\n            for c in range(self.n_classes):\n                if c not in obj: \n                    probs[:, c] = 0.\n                else: \n                    probs[:, c] = obj[c]\n            y_predicted_probs[ind] = probs    \n        assert (len(y_predicted_probs) == len(X))  \n        return y_predicted_probs",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "40b231b30fc9f58984904a569710f504",
          "grade": false,
          "grade_id": "cell-ac8fc52d8a39ccb4",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Test 0: Initialization (0.01 points)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# do not change this cell\nimport numpy as np\nimport unittest\nimport sys\nimport io\n\nfrom sklearn.datasets import fetch_california_housing, load_digits\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\ndigits_data = load_digits(n_class=2).data\ndigits_target = load_digits(n_class=2).target[:, None]\n\n",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3dee5ee83e0f63188671f08b57d70804",
          "grade": true,
          "grade_id": "cell-3473b7b6ffd64d07",
          "locked": true,
          "points": 0.01,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Test 1: Make splits loops (0.1 points)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X = np.ones((4, 5), dtype=float) * np.arange(4)[:, None]\ny = np.arange(4)[:, None] + np.asarray([0.2, -0.3, 0.1, 0.4])[:, None]\nclass_estimator = DecisionTree(max_depth=5, criterion_name='gini')\n\n(X_l, y_l), (X_r, y_r) = class_estimator.make_split(1, 1., X, y)\n\nflag_X = np.array_equal(X[:1], X_l) and np.array_equal(X[1:], X_r) \nflag_y = np.array_equal(y[:1], y_l) and np.array_equal(y[1:], y_r)\nassert flag_X and flag_y",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "48b2963c650791df41dfbd190ed247fd",
          "grade": true,
          "grade_id": "cell-e3503c286039ec55",
          "locked": true,
          "points": 0.09,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Test 2: Gini accuracy (0.2 points)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class_estimator = DecisionTree(max_depth=5, criterion_name='gini')\nclass_estimator.fit(X_train, y_train)\nans = class_estimator.predict(X_test)\naccuracy_gini = accuracy_score(y_test, ans)\nassert 0.96 < accuracy_gini",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4a2a5e274d8d866e1242a339a7751642",
          "grade": true,
          "grade_id": "cell-e2c4124a6f815118",
          "locked": true,
          "points": 0.2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Test 3: Entropy accuracy (0.2 points)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class_estimator = DecisionTree(max_depth=5, criterion_name='entropy')\nclass_estimator.fit(X_train, y_train)\nans = class_estimator.predict(X_test)\naccuracy = accuracy_score(y_test, ans)\nassert 0.96 < accuracy",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b3b167fd8a4950ffe1f1b8f691ab7f91",
          "grade": true,
          "grade_id": "cell-69473387a23d8dff",
          "locked": true,
          "points": 0.2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Test 4: Entropy probabilities (0.2 points)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class_estimator = DecisionTree(max_depth=10, criterion_name='entropy')\nclass_estimator.fit(X_train, y_train)\nans = class_estimator.predict(X_test)\nreference = np.array([0.48611111, 0.51388889])\nassert np.abs(np.sum(class_estimator.predict_proba(X_test).mean(axis=0) - reference)) < 1e-6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2ea5402e3fc351fe4bdc17dc7d48b591",
          "grade": true,
          "grade_id": "cell-e5f59af66e6c111b",
          "locked": true,
          "points": 0.2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Test 5: MSE mad_median (0.15 points)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "housing = fetch_california_housing()\nrandom_indices = np.random.choice(np.arange(len(housing.data)), 500)\n\nregr_data = housing.data[random_indices]\nregr_target = housing.target[random_indices, None]\nRX_train, RX_test, Ry_train, Ry_test = train_test_split(regr_data, regr_target, test_size=0.2, random_state=42)\n\nregressor = DecisionTree(max_depth=8, criterion_name='mad_median')\nregressor.fit(RX_train, Ry_train)\npredictions_mad = regressor.predict(RX_test)\nmse = mean_squared_error(Ry_test, predictions_mad)\nassert 0.4 < mse < 2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "740411f734d1c9841d5fcc124eb129d1",
          "grade": true,
          "grade_id": "cell-1a9c1e3609ed9aab",
          "locked": true,
          "points": 0.15,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Test 6: MSE Variance (0.15 points)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "housing = fetch_california_housing()\nrandom_indices = np.random.choice(np.arange(len(housing.data)), 500)\n\nregr_data = housing.data[random_indices]\nregr_target = housing.target[random_indices, None]\nRX_train, RX_test, Ry_train, Ry_test = train_test_split(regr_data, regr_target, test_size=0.2, random_state=42)\n\nregressor = DecisionTree(max_depth=8, criterion_name='variance')\nregressor.fit(RX_train, Ry_train)\npredictions_mad = regressor.predict(RX_test)\nmse = mean_squared_error(Ry_test, predictions_mad)\nassert 0.5 < mse < 1.8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d8ae6da572b6c3a76405ebfa4b9f4fd6",
          "grade": true,
          "grade_id": "cell-1ddb0377b6c68deb",
          "locked": true,
          "points": 0.15,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}